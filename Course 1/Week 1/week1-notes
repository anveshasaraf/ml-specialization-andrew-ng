Overview of Machine Learning

Formal Definition: Field of study that gives computers the ability to learn without being explicitly programmed. (Arthur Samuel, 1959)
Arthur Samuel - worked on the checker problem (used ML to learn how to play the game)




Supervised vs. Unsupervised Machine Learning

Types of ML Algorithms - 
1) Supervised Learning (most real-world applications, most rapid advancements)
2) Unsupervised Learning
3) Recommender Systems
4) Reinforcement Learning

(Note: AGI: Artificial General Intelligence: AI possessing human-level intelligence: long way to go but based on the principle of learning algorithms.)

Supervised Learning - algorithms that learn input to output mappings; learns from right answers.
(Most lucrative form is currently used in online advertising)

1) Regression - predict a number from infinitely many possible outputs (Housing Price prediction - has sizes and prices, predicts for a size not given already)
2) Classification - predict categories which are numeric/ non-numeric; small in number; can have two or more inputs (Breast cancer detection using medical records - malignant/ benign tumors - 0/1)

Unsupervised Learning - data comes with inputs x, but no output labels y; algorithm has to find structure in the data.
(Used in applications like Google News - clustering algorithm for news recommendations)

1) Clustering - forms clusters by grouping similar data points together.
2) Anomaly detection - find unusual data points.
3) Dimensionality reduction - compress data using fewer numbers.



Regression Model

Linear Regression Model (one of the most widely used models) - regression models predict numbers.

Useful Terminology -
i) Training Set: Data used to train the model
- x = "input" variable/ feature
- y = "output"/"target" variable
- m = number of training examples
- (x, y) = single training example
- (x superscript (i), y superscript (i)) = ith training example

After receiving the trainining set, the learning algorithm generates a function that takes an input x and generates a prediction "y-hat" while y is the actual target value in the dataset.
Representation of the function f: f subscript(w,b)(x) / f(x) = wx + b (w and b are parameters/ coefficients/ weights of the model)

(Note: Linear regression with one variable is also called univariate linear regression.)

Cost function formula

Note: y hat superscript (i) = f subscript w,b (x superscript (i))
f subscript w,b (x superscript (i)) = wx superscript (i) + b

Goal: Find w, b such that y hat superscript (i) is close to y superscript (i) for all (x superscript (i), y superscript (i)).
 
Error/ Cost function/ J(w,b) : 1/ 2m Summation i to m (y hat superscript (i) - y superscript (i))^ 2 where m = number of training examples (squared error cost function)
= 1/ 2m Summation i to m (f subscript w,b (x superscript (i))) - y superscript (i))^ 2

Simplifiied f subscript w (x) = wx - find a w that minimizes J(w)

